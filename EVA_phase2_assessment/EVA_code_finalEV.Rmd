---
title: "Ecological valuation assessment"
author: "Marie-Catherine Bouquieaux, Willem Boone, Hanneloor Heynderickx"
date: "22/03/2024"
output:
  pdf_document: 
    toc: true
html_document: default
editor_options: 
  markdown: 
    wrap: 60
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, tidy.opts=list(width.cutoff=45),tidy=TRUE)
```

#### Project: MARBEFES

#### Copyright (C) 2024 by VLIZ

**Contact: Marie-Catherine Bouquieaux ([marie.catherine.bouquieaux\@vliz.be](mailto:marie.catherine.bouquieaux@vliz.be){.email})**\
**Vlaams Instituut voor de Zee/Flanders Marine Institute**\
**Jacobsenstraat 1**\
**8400 Oostende, Belgium**

## 1. Install & load required libraries

If you like to knit the Rmarkdown script into a PDF an additional installation of a LaTeX program is required. Option to use tinytex::install_tinytex(). In addition, this Rmarkdown was created and ran using the following version:

-   R: 4.3.2

-   Rstudio: 2023.12.1

-   Computer: Windows 11

-   Package: last version available on the 08 of March 2024, except for ggplot2, for which the version is specified below

*List of packages used*
```{r list_package, message=FALSE, warning=FALSE}
my_packages <- c( "data.table","sf", "raster",   
                 "tidyr","dplyr", "rnaturalearth","rnaturalearthdata", "devtools", "knitr","kableExtra") 
```

*To run only if some packages mentioned above are not already installed*
```{r install_package}
install.packages(my_packages)
# Latest update of ggplot2 is not working properly
devtools::install_version("ggplot2", version = "3.4.4")
```

*Load packages*
```{r load_package, message=FALSE, warning=FALSE}
lapply(c(my_packages, "ggplot2"), require, character.only = TRUE)
```

## 2. Input parameters (Settings)

###2.1 Define the working environment

**Input**: Filepath containing the shapefile **and** shx, .dbf and .prj for each Ecological Component.

**Output directory**: Filepath for output files; make sure directory exists.

**list_EC_result**: List of the files with the EC results; please add the extension of the file as well (for example "EVA_EC_bird_layers.shp")

**xmin/xmax**: Minimum and maximum longitude value to be used as limit on the map; must be provided in degrees.

**ymin/ymax**: Minimum and maximum latitude value to be used as limit on the map; must be provided in degrees.

**quantile_classification**: Please indicate TRUE if you prefer to utilize a distribution-based method for classifying your results, where the categories are determined by quantiles (20,40,60,80,100). If FALSE is enter, specific categories will be used and can be changed/defined in point 7. 

```{r setting1}
## Path settings for input and output data
Input<-"~/Documents/EVA_process/output/"
Output<-"~/Documents/EVA_process/final_EV/"

# List of files: 
list_EC_result_3km <- c("EVA_EC_bird_layers.shp","EVA_EC_fish_layers.shp","EVA_EC_mammal_layers.shp","EVA_EC_phytoplankton_layers.shp","EVA_EC_zooplankton_layers.shp")
list_EC_result_250m <- c("EVA_EC_benthos_layers.shp")

#Max/min latitude and longitude
xmin <- 1.5
xmax <- 3.5
ymin <- 51
ymax <- 55.5 


## Path settings for the coordinates of the BBT
# Complete path and file name containing the BBT information
BBT_coordinates <- "~/Documents/EVA_process/BBT_coordinates/centerlinev2.gpkg"
# If a specific layers need to be used, enter name here:
BBT_layer <- "buffer20km"

# Which classification methods should be used:
quantile_classification <- TRUE
```

### 2.2 Get surrounding country map information

Change x and y limit to what is required for your BBT.

```{r world_data}
world_data <- ne_countries(scale = "medium", returnclass = "sf") |>
  sf::st_transform(crs="EPSG:4326") |>
  sf::st_crop(c(xmin= xmin, xmax = xmax, ymin = ymin, ymax = ymax))
```

```{r qlassification}
# Classification the results into five categories
# x is the datafrale and y is the column; z is the name you want the added column to have.
# You can modify the categories based on the range of variability of your data. 
if (quantile_classification == TRUE){
  QClassify <- function(x, y, z) { 
  q<-quantile(y,  probs = c(20, 40, 60, 80, 100)/100,na.rm= TRUE)
  x <- x %>%
    mutate(class_1_5 = ifelse(y >= 0 & y <= q[1], 1,
                        ifelse(y > q[1] & y <= q[2], 2,
                               ifelse(y > q[2] & y <= q[3], 3,
                                      ifelse(y > q[3] & y <= q[4], 4,
                                             ifelse(y > q[4] & y <= q[5], 5, NA))))))
  names(x)[names(x) == "class_1_5"] <- z
  return(x)
  }
} else{
  QClassify <- function(x, y, z) { 
    x <- x %>% 
    mutate(class_1_5 = case_when((y >=0 & y <= 1.5) ~ 1,
                               (y >1.5 & y <= 2.5) ~ 2,
                               (y >2.5 & y <= 3.5) ~ 3,
                               (y >3.5 & y <= 4.5) ~ 4,
                               (y >4.5 & y <= 5) ~ 5,))
  names(x)[names(x) == "class_1_5"] <- z
  return(x)
  }
}

```

## 3. Rescaling of the 250m grid

If you have EC that have been calculated using a 250m grid, please run the following code to resize it to a 3km grid.

### Resize the smaller grid

```{r}
if ( any(nchar(list_EC_result_250m) > 0 )){
  # You should have specify the path/name of your file above, as well as the layer name is any is present
  if (nchar(BBT_layer) > 0){
    polygon_bbt <- st_read(BBT_coordinates, layer = BBT_layer)
  } else {
    polygon_bbt <- st_read(BBT_coordinates)
  }
  # Change projection of the coordinates to ease the grid calculation
  Polygon_coordinated_32631 <- st_transform(polygon_bbt, crs = st_crs(32631))
  
  
  # 250m grid
  GRD_250 <- st_make_grid(Polygon_coordinated_32631$geom, cellsize = 250, square = FALSE)
  # Create dataframe with both the grid polygon and grid id for each subzones
  grid_polygons_sf_250 <- st_sf(grid_id = seq_along(GRD_250[Polygon_coordinated_32631$geom]),
                            geometry = GRD_250[Polygon_coordinated_32631$geom], crs = 32631)
  # 3km grid
  GRD_3km <- st_make_grid(Polygon_coordinated_32631$geom, cellsize = 3000, square = FALSE)
  # Create dataframe with both the grid polygon and grid id for each subzones
  grid_polygons_sf_3km <- st_sf(grid_id_3km = seq_along(GRD_3km[Polygon_coordinated_32631$geom]),
                            geometry = GRD_3km[Polygon_coordinated_32631$geom], crs = 32631)
  
  hexagons_250 <- st_transform(grid_polygons_sf_250, crs = "EPSG:4326")
  hexagons_3000 <- st_transform(grid_polygons_sf_3km, crs = "EPSG:4326")
  
  inside_grid <- st_join(st_centroid(hexagons_250),hexagons_3000, join = st_intersects) #250m grid associated to one 3km hexagon
  inside_grid <- st_drop_geometry(inside_grid) 
  
  
  data_EC_rescaled <- data.frame()
  
  for (j in length(list_EC_result_250m)){
    data_to_rescale <- st_read(paste0(Input,list_EC_result_250m[j]))
    data_rescaled <- data_to_rescale %>%
      dplyr::select(grid_id, Total) %>%
      inner_join(inside_grid, by = "grid_id") %>% 
      st_drop_geometry() %>% 
      group_by(grid_id_3km) %>%
      summarise(Total_3k = mean(Total, na.rm = TRUE)) %>%
      mutate_all(~ifelse(is.nan(.), NA, .)) %>% 
      drop_na(grid_id_3km) %>%
      full_join(grid_polygons_sf_3km, by = "grid_id_3km") %>%
      rename(grid_id = grid_id_3km)
    
    
    data_rescaled_classified <- QClassify(data_rescaled,data_rescaled$Total_3k, "Total")
    data_rescaled_classified$EC <- list_EC_result_250m[j]
    data_rescaled_classified <- data_rescaled_classified[,c("grid_id","Total","EC", "geometry")]
    data_EC_rescaled <- rbind(data_EC_rescaled, data_rescaled_classified)
  }
}

```

## 4. Computation of the final EV score 

```{r score_combine}

### Load shapefile with the results from each EC ###

data_EC <- data.frame()
for(i in 1:length(list_EC_result_3km)){
  data_current <- st_read(paste0(Input,list_EC_result_3km[i]))
  data_current <- data_current[,c("grid_id","Total","geometry")]
  data_current$EC <- list_EC_result_3km[i]
  data_EC <- rbind(data_EC,data_current) 
}

if(exists('data_EC_rescaled') && is.data.frame(get('data_EC_rescaled'))){
  
  data_geometry <- st_as_sf(data_EC_rescaled)
  st_crs(data_geometry) <- st_crs(data_EC)
  data_EC <- rbind(data_EC,data_geometry)
}

data_EC_total_valuation <- data_EC %>%
  st_drop_geometry() %>%
  group_by(grid_id) %>% 
  dplyr::summarise_at(vars(Total), list(total_Valuation = mean), .groups="drop", na.rm=TRUE)

grid_polygons_sf <- data_EC[unique(data_EC$grid_id),c( "grid_id", "geometry")]


BBT_EC_EVa<-QClassify(data_EC_total_valuation, data_EC_total_valuation$total_Valuation, "Total_EV")

BBT_EC_EV <- BBT_EC_EVa %>%
  mutate_all(~ifelse(is.nan(.), NA, .)) %>%
  inner_join(grid_polygons_sf, by = "grid_id")

# Export the results to a shapefile
st_write(BBT_EC_EV, file.path(Output,"BBT_final_EV_layers.shp"), append = FALSE)


```

## 5. Map of the EV score

**Plot EV score**

```{r plot_final+score, message=FALSE, warning=FALSE, results='hide'}
ggplot()+
  geom_sf(data = BBT_EC_EV$geometry, aes(fill = factor(BBT_EC_EV$Total_EV)), lwd = 0, colour = "grey90") +
  scale_fill_manual(values = viridisLite::viridis(5), name = "Ecological Valuation score", limits = c(1,2,3,4,5), na.value = "grey94", labels = c(
    "1" = "Very Low",
    "2" = "Low",
    "3" = "Medium",
    "4" = "High",
    "5" = "Very High"
  ), drop = FALSE) +
  geom_sf(data = world_data$geometry) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(),
        axis.text.x=element_text(angle=300,hjust=0))

ggsave(filename=file.path(Output,"Total_Valutation_Score.png"),
      width = NA,
      height = NA,
      units = "cm",
      dpi=1600)

```
